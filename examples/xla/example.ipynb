{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xla_example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMhkSk2cPOhMswQcHTGEJxs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ditwoo/batteries/blob/master/examples/xla/example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kntChZQeSLHV",
        "colab_type": "text"
      },
      "source": [
        "### Installing required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buAm1u5mRypU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f10c2576-7857-448d-ad41-bc9dd029d87e"
      },
      "source": [
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.6-cp36-cp36m-linux_x86_64.whl > /dev/null\n",
        "!pip install git+https://github.com/ditwoo/batteries > /dev/null"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Running command git clone -q https://github.com/ditwoo/batteries /tmp/pip-req-build-7off7k8v\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkJ06iYPSTt1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51ffe620-c640-40c6-d404-d69c859b69f5"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"XLA_USE_BF16\"] = \"1\"\n",
        "os.environ[\"XLA_TENSOR_ALLOCATOR_MAX_SIZE\"] = \"100000000\"\n",
        "\n",
        "import shutil\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data.distributed as dist\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import Compose, Normalize, ToTensor\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "\n",
        "from batteries import (\n",
        "    seed_all,\n",
        "    CheckpointManager,\n",
        "    TensorboardLogger,\n",
        "    t2d,\n",
        "    make_checkpoint,\n",
        ")\n",
        "from batteries.progress import tqdm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:TPU has started up successfully with version pytorch-1.6\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwKS9FkfQbbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reduce_fn(vals):\n",
        "    return sum(vals) / len(vals)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxj2q5ikUrJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_transforms(dataset: str):\n",
        "    \"\"\"Get transforms depends from dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset (str): dataset type (train or valid)\n",
        "\n",
        "    Returns:\n",
        "        dataset transforms\n",
        "    \"\"\"\n",
        "    return Compose([\n",
        "        ToTensor(),\n",
        "        # imagenet:\n",
        "        # Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        # cifar100:\n",
        "        Normalize(mean=(0.5071, 0.4867, 0.4408), std=(0.2675, 0.2565, 0.2761)),\n",
        "    ])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWA2CjrhX1Bo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_fn(\n",
        "    model,\n",
        "    loader,\n",
        "    device,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    scheduler=None,\n",
        "    accum_steps: int = 1,\n",
        "):\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    to_iter = enumerate(loader)\n",
        "\n",
        "\n",
        "    for _idx, (bx, by) in enumerate(loader):\n",
        "        bx, by = t2d((bx, by), device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(bx)\n",
        "\n",
        "        loss = loss_fn(outputs, by)\n",
        "        _loss = loss.item()\n",
        "        losses.append(_loss)\n",
        "        loss.backward()\n",
        "\n",
        "        if (_idx + 1) % accum_steps == 0:\n",
        "            # optimizer.step()\n",
        "            xm.optimizer_step(optimizer)\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "\n",
        "    metrics = {\n",
        "        \"loss\": np.mean(losses),\n",
        "    }\n",
        "    return metrics"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IkpND0HYIu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def valid_fn(model, loader, device, loss_fn):\n",
        "    model.eval()\n",
        "\n",
        "    losses = []\n",
        "    num_correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad() as progress:\n",
        "        to_iter = loader\n",
        "        for bx, by in loader:\n",
        "            bx, by = t2d((bx, by), device)\n",
        "            \n",
        "            outputs = model(bx)\n",
        "            \n",
        "            loss = loss_fn(outputs, by).item()\n",
        "            losses.append(loss)\n",
        "\n",
        "            num_correct += torch.eq(\n",
        "                by.flatten().detach(),\n",
        "                outputs.argmax(1).flatten().detach()\n",
        "            ).sum().item()\n",
        "            total += bx.size(0)\n",
        "\n",
        "    dataset_acc = num_correct / total\n",
        "    metrics = {\n",
        "        \"loss\": np.mean(losses),\n",
        "        \"accuracy\": dataset_acc,\n",
        "    }\n",
        "    return metrics"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plDeTLSNigN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_metrics(stage: str, metrics: dict, loader: str, epoch: int) -> dict:\n",
        "    \"\"\"Write metrics to tensorboard and stdout.\n",
        "    Args:\n",
        "        stage (str): stage name\n",
        "        metrics (dict): metrics computed during training/validation steps\n",
        "        loader (str): loader name\n",
        "        epoch (int): epoch number\n",
        "    \n",
        "    Returns:\n",
        "        dict with reduced metrics\n",
        "    \"\"\"\n",
        "    order = (\"loss\", \"accuracy\")\n",
        "    line = []\n",
        "    reduced_metrics = {}\n",
        "    for metric_name in order:\n",
        "        # loaders have different metrics\n",
        "        if metric_name in metrics:\n",
        "            value = xm.mesh_reduce(metric_name, metrics[metric_name], reduce_fn)\n",
        "            reduced_metrics[metric_name] = value\n",
        "            line.append(f\"{metric_name:>10}: {value:.4f}\")\n",
        "    s = f\"{loader}:\\n\" + \"\\n\".join(line)\n",
        "    xm.master_print(s)\n",
        "    return reduced_metrics"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOhMk7ttiPpf",
        "colab_type": "text"
      },
      "source": [
        "### Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgMuN52lc-ZZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ccb3c706-a79d-4dfa-ebb3-14f0fe12a704"
      },
      "source": [
        "# make datasets/dataloaders\n",
        "transforms = get_transforms(\"\")\n",
        "\n",
        "train_dataset = CIFAR10(\n",
        "    \"/tmp/CIFAR\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms\n",
        ")\n",
        "\n",
        "test_dataset = CIFAR10(\n",
        "    \"/tmp/CIFAR\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKozq2K7T9fu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def experiment(index, flags):\n",
        "    # prepare for training\n",
        "    seed_all(flags.get(\"seed\", 1234))\n",
        "    device = xm.xla_device()\n",
        "\n",
        "    global train_dataset, test_dataset\n",
        "    \n",
        "    # # Downloads train and test datasets\n",
        "    # # Note: master goes first and downloads the dataset only once (xm.rendezvous)\n",
        "    # #       all the other workers wait for the master to be done downloading.\n",
        "    # if not xm.is_master_ordinal():\n",
        "    #     xm.rendezvous('download_only_once')\n",
        "\n",
        "    # if xm.is_master_ordinal():\n",
        "    #     xm.rendezvous('download_only_once')\n",
        "\n",
        "    train_sampler = dist.DistributedSampler(\n",
        "        train_dataset,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=True,\n",
        "        seed=1234,\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=flags.get(\"train_batch_size\", 64),\n",
        "        sampler=train_sampler,\n",
        "        num_workers=flags.get(\"num_workers\", 8),\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    test_sampler = dist.DistributedSampler(\n",
        "        test_dataset,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=False,\n",
        "        seed=1234,\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=flags.get(\"test_batch_size\", 64),\n",
        "        sampler=test_sampler,\n",
        "        num_workers=flags.get(\"num_workers\", 8),\n",
        "    )\n",
        "\n",
        "    # general instructions\n",
        "    main_metric = \"accuracy\"\n",
        "    minimize_metric = False\n",
        "\n",
        "    stage = \"stage_0\"\n",
        "    n_epochs = flags.get(\"num_epochs\", 5)\n",
        "\n",
        "    checkpointer = CheckpointManager(\n",
        "        logdir=os.path.join(flags.get(\"logdir\", \".\"), stage),\n",
        "        metric=main_metric,\n",
        "        metric_minimization=minimize_metric,\n",
        "        save_n_best=3,\n",
        "        save_fn=xm.save,\n",
        "    )\n",
        "\n",
        "    model = resnet18(\n",
        "        pretrained=False,\n",
        "        progress=False,\n",
        "        num_classes=10\n",
        "    )\n",
        "    model = model.to(device)\n",
        "    learning_rate = 1e-3 # * xm.xrt_world_size()\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(), \n",
        "        **flags.get(\"optimizer\", {\"lr\": 1e-3}),\n",
        "    )\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
        "        xm.master_print(f\"[{current_time}]\\n[Epoch {epoch}/{n_epochs}]\")\n",
        "\n",
        "        para_train_loader = (\n",
        "            pl.ParallelLoader(train_loader, [device]).per_device_loader(device)\n",
        "        )\n",
        "        train_metrics = train_fn(\n",
        "            model, para_train_loader, device, criterion, optimizer\n",
        "        )\n",
        "        reduced_train_metrics = log_metrics(\n",
        "            stage, train_metrics, \"train\", epoch\n",
        "        )\n",
        "        \n",
        "        para_test_loader = (\n",
        "            pl.ParallelLoader(test_loader, [device]).per_device_loader(device)\n",
        "        )\n",
        "        valid_metrics = valid_fn(\n",
        "            model, para_test_loader, device, criterion,\n",
        "        )\n",
        "        reduced_valid_metrics = log_metrics(\n",
        "            stage, valid_metrics, \"valid\", epoch\n",
        "        )\n",
        "        xm.master_print(\"\")\n",
        "\n",
        "        checkpointer.process(\n",
        "            metric_value=reduced_valid_metrics[main_metric],\n",
        "            epoch=epoch,\n",
        "            checkpoint=make_checkpoint(\n",
        "                stage, epoch, model, \n",
        "                metrics={\n",
        "                    \"train\": reduced_train_metrics,\n",
        "                    \"valid\": reduced_valid_metrics,\n",
        "                },\n",
        "            )\n",
        "        )\n",
        "\n",
        "        scheduler.step()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKet1BRLa4bd",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtG_6Rvza2PL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4d37c9ff-2b68-4b04-b3f0-b7fc8443346a"
      },
      "source": [
        "logdir = \"./logs/\"\n",
        "\n",
        "if os.path.isdir(logdir):\n",
        "    shutil.rmtree(logdir, ignore_errors=True)\n",
        "    print(f\"* Removed existing '{logdir}' directory!\")\n",
        "\n",
        "flags = {\n",
        "    \"logdir\": logdir,\n",
        "    \"seed\": 321,\n",
        "    \"train_batch_size\": 128,\n",
        "    \"test_batch_size\": 256,\n",
        "    \"num_workers\": 8,\n",
        "    \"num_epochs\": 10,\n",
        "    \"optimizer\": {\n",
        "        \"lr\": 1e-3,\n",
        "    },\n",
        "}\n",
        "\n",
        "xmp.spawn(experiment, args=(flags,), nprocs=8, start_method='fork')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "* Removed existing './logs/' directory!\n",
            "[2020-09-18 11:32:55]\n",
            "[Epoch 1/10]\n",
            "train:\n",
            "      loss: 1.5179\n",
            "valid:\n",
            "      loss: 1.3156\n",
            "  accuracy: 0.5279\n",
            "\n",
            "[2020-09-18 11:33:29]\n",
            "[Epoch 2/10]\n",
            "train:\n",
            "      loss: 1.0833\n",
            "valid:\n",
            "      loss: 1.1822\n",
            "  accuracy: 0.5891\n",
            "\n",
            "[2020-09-18 11:33:55]\n",
            "[Epoch 3/10]\n",
            "train:\n",
            "      loss: 0.8676\n",
            "valid:\n",
            "      loss: 1.0962\n",
            "  accuracy: 0.6168\n",
            "\n",
            "[2020-09-18 11:34:23]\n",
            "[Epoch 4/10]\n",
            "train:\n",
            "      loss: 0.7172\n",
            "valid:\n",
            "      loss: 1.2264\n",
            "  accuracy: 0.5922\n",
            "\n",
            "[2020-09-18 11:34:49]\n",
            "[Epoch 5/10]\n",
            "train:\n",
            "      loss: 0.5896\n",
            "valid:\n",
            "      loss: 1.3979\n",
            "  accuracy: 0.5920\n",
            "\n",
            "[2020-09-18 11:35:15]\n",
            "[Epoch 6/10]\n",
            "train:\n",
            "      loss: 0.4582\n",
            "valid:\n",
            "      loss: 1.2373\n",
            "  accuracy: 0.6400\n",
            "\n",
            "[2020-09-18 11:35:41]\n",
            "[Epoch 7/10]\n",
            "train:\n",
            "      loss: 0.2949\n",
            "valid:\n",
            "      loss: 1.2663\n",
            "  accuracy: 0.6490\n",
            "\n",
            "[2020-09-18 11:36:08]\n",
            "[Epoch 8/10]\n",
            "train:\n",
            "      loss: 0.1502\n",
            "valid:\n",
            "      loss: 1.1846\n",
            "  accuracy: 0.6789\n",
            "\n",
            "[2020-09-18 11:36:34]\n",
            "[Epoch 9/10]\n",
            "train:\n",
            "      loss: 0.0699\n",
            "valid:\n",
            "      loss: 1.1767\n",
            "  accuracy: 0.6826\n",
            "\n",
            "[2020-09-18 11:37:01]\n",
            "[Epoch 10/10]\n",
            "train:\n",
            "      loss: 0.0515\n",
            "valid:\n",
            "      loss: 1.1799\n",
            "  accuracy: 0.6830\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ULtbCbjbdby",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "324e08df-2f8b-464b-9dfe-4e32bb4f7203"
      },
      "source": [
        "!ls -la logs/stage_0/"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 218792\n",
            "drwxr-xr-x 2 root root     4096 Sep 18 11:37 .\n",
            "drwxr-xr-x 3 root root     4096 Sep 18 11:33 ..\n",
            "-rw-r--r-- 1 root root 44802903 Sep 18 11:37 best.pth\n",
            "-rw-r--r-- 1 root root 44802903 Sep 18 11:37 exp_10.pth\n",
            "-rw-r--r-- 1 root root 44802903 Sep 18 11:36 exp_8.pth\n",
            "-rw-r--r-- 1 root root 44802903 Sep 18 11:37 exp_9.pth\n",
            "-rw-r--r-- 1 root root 44802903 Sep 18 11:37 last.pth\n",
            "-rw-r--r-- 1 root root      648 Sep 18 11:37 metrics.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MImaeuIf6JH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5cc60194-63d4-4bb5-ae9a-d8208f59518c"
      },
      "source": [
        "from batteries import load_checkpoint\n",
        "\n",
        "model = resnet18(\n",
        "    pretrained=False,\n",
        "    progress=False,\n",
        "    num_classes=10\n",
        ")\n",
        "load_checkpoint(\"./logs/stage_0/best.pth\", model)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<= Loaded model from './logs/stage_0/best.pth'\n",
            "Stage: stage_0\n",
            "Epoch: 10\n",
            "Checkpoint metrics:\n",
            "{'train': {'loss': 0.05153878529866536}, 'valid': {'loss': 1.1798828125, 'accuracy': 0.6829999999999999}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhN5WCqOA4g_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}